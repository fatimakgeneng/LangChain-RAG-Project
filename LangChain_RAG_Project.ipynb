{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6eLfH8fUDrO+dwr6YPS5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "918e607cb8354222a28a044286bd09f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8af7992ffdc4e00910df3306c3556bc",
              "IPY_MODEL_35e002c4bf6d4304b2deb98054bad0df",
              "IPY_MODEL_305cad1653314853b84ebe9b3c6a8051"
            ],
            "layout": "IPY_MODEL_e19214bf7d694de98522371b243790f9"
          }
        },
        "b8af7992ffdc4e00910df3306c3556bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e97fbd743784156bcc4bc848df871b7",
            "placeholder": "​",
            "style": "IPY_MODEL_837c621da5774d57abe29575f1b00b29",
            "value": "100%"
          }
        },
        "35e002c4bf6d4304b2deb98054bad0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5f9e795fe5d4b15b51da816e8815689",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_118ab70dd9c5480cb483a9934d206ab9",
            "value": 111898327
          }
        },
        "305cad1653314853b84ebe9b3c6a8051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a836e671a3f4ac8bf6f025d33d42211",
            "placeholder": "​",
            "style": "IPY_MODEL_77cb24b4de064e00bab1da1a4de6c26c",
            "value": " 107M/107M [00:00&lt;00:00, 188MB/s]"
          }
        },
        "e19214bf7d694de98522371b243790f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e97fbd743784156bcc4bc848df871b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "837c621da5774d57abe29575f1b00b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5f9e795fe5d4b15b51da816e8815689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118ab70dd9c5480cb483a9934d206ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a836e671a3f4ac8bf6f025d33d42211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77cb24b4de064e00bab1da1a4de6c26c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatimakgeneng/LangChain-RAG-Project/blob/main/LangChain_RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Pinecone**"
      ],
      "metadata": {
        "id": "1ypVrDnY-XaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n4KjtjXHSVsq",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979dd310-8f6f-4a2a-b8b7-2d7c5be83449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/454.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q -U python-dotenv langchain pinecone-client google-generativeai openai tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve and set the environment variables\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_ENVIRONMENT'] = userdata.get('PINECONE_ENVIRONMENT')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "Ozh1fjZ-RMFN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'], environment=os.environ['PINECONE_ENVIRONMENT'])"
      ],
      "metadata": {
        "id": "NHFhJlnxoztw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to an existing index\n",
        "index_name = \"gemini-rag-index\"  # Replace with your actual index name\n",
        "index = pc.Index(index_name)\n",
        "print(f\"Successfully connected to index: {index_name}\")"
      ],
      "metadata": {
        "id": "ds3uKipsp7lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7deba0-fac4-4fef-80a0-5e7a3780a1d9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to index: gemini-rag-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain-google-genai\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "ihz0p83zp7UJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "12b2fe2a-5c16-41b8-bd9a-6fb78152bd90"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain-community\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents\n",
        "loader = TextLoader(\"/content/document.txt\")  # Replace with your file\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "HCa_vvE5sVeB",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ca50d44-0e21-495e-e538-06eb112aa675"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m147.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "    # Pass metadata as a dictionary\n",
        "    index.upsert([(doc.metadata[\"source\"], vector, {\"text\": doc.page_content})])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ScoJXpatvcP",
        "outputId": "5b63b230-263d-4b74-f291-bfd214242cce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:01<00:00,  3.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "retriever = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings, text_key=\"text\")"
      ],
      "metadata": {
        "id": "yQF6vA03tvN2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain-google-genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(api_key=os.environ.get(\"GOOGLE_API_KEY\"),model=\"gemini-1.5-flash\", temperature=0.7)"
      ],
      "metadata": {
        "id": "kKf8L3lbyN0-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores.base import VectorStoreRetriever\n",
        "\n",
        "# Create a VectorStoreRetriever from your Pinecone index\n",
        "retriever = VectorStoreRetriever(vectorstore=Pinecone.from_existing_index(index_name=index_name, embedding=embeddings, text_key=\"text\"))\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_model,\n",
        "    chain_type=\"stuff\",  # Other options: \"map_reduce\", \"refine\"\n",
        "    retriever=retriever)"
      ],
      "metadata": {
        "id": "B7zmqMFIyW3k"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what happens when you integrate the AI in genetics\"\n",
        "response = qa_chain.run(query)\n",
        "print(f\"Fatima's Question: {query}\")\n",
        "print()\n",
        "print(f\"LLM's Response: {response}\")"
      ],
      "metadata": {
        "id": "QUp1uKGHyWjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff56643-8d01-4b42-998f-ea8ab42e0923"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fatima's Question: what happens when you integrate the AI in genetics\n",
            "\n",
            "LLM's Response: Integrating AI into genetics has the potential to improve human health and solve critical challenges.  However, responsible innovation, collaboration, and regulation are crucial for its success.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Few important commands** such as list of models, dimentions & functions of pinecone"
      ],
      "metadata": {
        "id": "SRiOjAFTCMpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "list(genai.list_models())   # models to choose from"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1Yalkx9CgLN",
        "outputId": "c6757373-c69f-48bd-8398-9f855e25ac0a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The original Gemini 1.0 Pro model. This model will be discontinued on '\n",
              "                    'February 15th, 2025. Move to a newer Gemini version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 '\n",
              "                    'Pro will be discontinued on February 15th, 2025. Move to a newer Gemini '\n",
              "                    'version.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
              "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
              "                    'Move to a newer Gemini version.'),\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
              "                    'million tokens.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Pro 002',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
              "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
              "       input_token_limit=2000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-pro-exp-0801',\n",
              "       base_model_id='',\n",
              "       version='exp-0801',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
              "                    'across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001-tuning',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001 Tuning',\n",
              "       description=('Version of Gemini 1.5 Flash that supports tuning, our fast and versatile '\n",
              "                    'multimodal model for scaling across diverse tasks, released in May of 2024.'),\n",
              "       input_token_limit=16384,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
              "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-002',\n",
              "       base_model_id='',\n",
              "       version='002',\n",
              "       display_name='Gemini 1.5 Flash 002',\n",
              "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B 001',\n",
              "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
              "                    'Flash model, released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash-8B Latest',\n",
              "       description=('Alias that points to the most recent production (non-experimental) release '\n",
              "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
              "                    'released in October of 2024.'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
              "       description=('Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-1.5-flash-8b-exp-0924',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 8B Experimental 0924',\n",
              "       description=('Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our '\n",
              "                    'smallest and most cost effective Flash model. Replaced by '\n",
              "                    'Gemini-1.5-flash-8b-001 (stable).'),\n",
              "       input_token_limit=1000000,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Experimental',\n",
              "       description='Gemini 2.0 Flash Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-exp-1206',\n",
              "       base_model_id='',\n",
              "       version='exp_1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1121',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-exp-1114',\n",
              "       base_model_id='',\n",
              "       version='exp-1206',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (December 6th, 2024) of Gemini.',\n",
              "       input_token_limit=2097152,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
              "       description='Gemini 2.0 Flash Thinking Experimental',\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash Thinking Experimental',\n",
              "       description='Gemini 2.0 Flash Thinking Experimental',\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/learnlm-1.5-pro-experimental',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='LearnLM 1.5 Pro Experimental',\n",
              "       description=('Alias that points to the most recent stable version of Gemini 1.5 Pro, our '\n",
              "                    'mid-size multimodal model that supports up to 2 million tokens.'),\n",
              "       input_token_limit=32767,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(dir(retriever))  #List of functions in pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6NGtl6iKEOD",
        "outputId": "16baf8cd-fa49-45df-ca2d-c64e82507f4b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['InputType',\n",
              " 'OutputType',\n",
              " '__abstractmethods__',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__class_vars__',\n",
              " '__copy__',\n",
              " '__deepcopy__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__fields__',\n",
              " '__fields_set__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__get_pydantic_core_schema__',\n",
              " '__get_pydantic_json_schema__',\n",
              " '__getattr__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__iter__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__or__',\n",
              " '__orig_bases__',\n",
              " '__parameters__',\n",
              " '__pretty__',\n",
              " '__private_attributes__',\n",
              " '__pydantic_complete__',\n",
              " '__pydantic_computed_fields__',\n",
              " '__pydantic_core_schema__',\n",
              " '__pydantic_custom_init__',\n",
              " '__pydantic_decorators__',\n",
              " '__pydantic_extra__',\n",
              " '__pydantic_fields__',\n",
              " '__pydantic_fields_set__',\n",
              " '__pydantic_generic_metadata__',\n",
              " '__pydantic_init_subclass__',\n",
              " '__pydantic_parent_namespace__',\n",
              " '__pydantic_post_init__',\n",
              " '__pydantic_private__',\n",
              " '__pydantic_root_model__',\n",
              " '__pydantic_serializer__',\n",
              " '__pydantic_validator__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__replace__',\n",
              " '__repr__',\n",
              " '__repr_args__',\n",
              " '__repr_name__',\n",
              " '__repr_recursion__',\n",
              " '__repr_str__',\n",
              " '__rich_repr__',\n",
              " '__ror__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__signature__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_abatch_with_config',\n",
              " '_abc_impl',\n",
              " '_acall_with_config',\n",
              " '_aget_relevant_documents',\n",
              " '_atransform_stream_with_config',\n",
              " '_batch_with_config',\n",
              " '_calculate_keys',\n",
              " '_call_with_config',\n",
              " '_check_frozen',\n",
              " '_copy_and_set_values',\n",
              " '_expects_other_args',\n",
              " '_get_ls_params',\n",
              " '_get_relevant_documents',\n",
              " '_get_value',\n",
              " '_is_protocol',\n",
              " '_iter',\n",
              " '_new_arg_supported',\n",
              " '_transform_stream_with_config',\n",
              " 'aadd_documents',\n",
              " 'abatch',\n",
              " 'abatch_as_completed',\n",
              " 'add_documents',\n",
              " 'aget_relevant_documents',\n",
              " 'ainvoke',\n",
              " 'allowed_search_types',\n",
              " 'as_tool',\n",
              " 'assign',\n",
              " 'astream',\n",
              " 'astream_events',\n",
              " 'astream_log',\n",
              " 'atransform',\n",
              " 'batch',\n",
              " 'batch_as_completed',\n",
              " 'bind',\n",
              " 'config_schema',\n",
              " 'config_specs',\n",
              " 'configurable_alternatives',\n",
              " 'configurable_fields',\n",
              " 'construct',\n",
              " 'copy',\n",
              " 'dict',\n",
              " 'from_orm',\n",
              " 'get_config_jsonschema',\n",
              " 'get_graph',\n",
              " 'get_input_jsonschema',\n",
              " 'get_input_schema',\n",
              " 'get_lc_namespace',\n",
              " 'get_name',\n",
              " 'get_output_jsonschema',\n",
              " 'get_output_schema',\n",
              " 'get_prompts',\n",
              " 'get_relevant_documents',\n",
              " 'input_schema',\n",
              " 'invoke',\n",
              " 'is_lc_serializable',\n",
              " 'json',\n",
              " 'lc_attributes',\n",
              " 'lc_id',\n",
              " 'lc_secrets',\n",
              " 'map',\n",
              " 'metadata',\n",
              " 'model_computed_fields',\n",
              " 'model_config',\n",
              " 'model_construct',\n",
              " 'model_copy',\n",
              " 'model_dump',\n",
              " 'model_dump_json',\n",
              " 'model_extra',\n",
              " 'model_fields',\n",
              " 'model_fields_set',\n",
              " 'model_json_schema',\n",
              " 'model_parametrized_name',\n",
              " 'model_post_init',\n",
              " 'model_rebuild',\n",
              " 'model_validate',\n",
              " 'model_validate_json',\n",
              " 'model_validate_strings',\n",
              " 'name',\n",
              " 'output_schema',\n",
              " 'parse_file',\n",
              " 'parse_obj',\n",
              " 'parse_raw',\n",
              " 'pick',\n",
              " 'pipe',\n",
              " 'schema',\n",
              " 'schema_json',\n",
              " 'search_kwargs',\n",
              " 'search_type',\n",
              " 'stream',\n",
              " 'tags',\n",
              " 'to_json',\n",
              " 'to_json_not_implemented',\n",
              " 'transform',\n",
              " 'update_forward_refs',\n",
              " 'validate',\n",
              " 'validate_search_type',\n",
              " 'vectorstore',\n",
              " 'with_alisteners',\n",
              " 'with_config',\n",
              " 'with_fallbacks',\n",
              " 'with_listeners',\n",
              " 'with_retry',\n",
              " 'with_types']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vector)   # dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4Ws3s4MPC97",
        "outputId": "e6f356af-666d-42d7-cf8b-79d3a87bf4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Face detection using FaceNet & Milvus database**"
      ],
      "metadata": {
        "id": "P0sfbi4BUGrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq facenet-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyieFNxsIuN_",
        "outputId": "6f677ba5-6dc5-4971-94ba-1325e4a8d716"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K99II2GIXl_",
        "outputId": "bb15c7a0-0f07-4641-a5e7-873e225943d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/4.5 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m4.1/4.5 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires Pillow<10.3.0,>=10.2.0, but you have pillow 11.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "MI17ouTJbv4f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "# model   Optional (If wanna check the model details)"
      ],
      "metadata": {
        "id": "ur1jWQyXcWmu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "918e607cb8354222a28a044286bd09f0",
            "b8af7992ffdc4e00910df3306c3556bc",
            "35e002c4bf6d4304b2deb98054bad0df",
            "305cad1653314853b84ebe9b3c6a8051",
            "e19214bf7d694de98522371b243790f9",
            "9e97fbd743784156bcc4bc848df871b7",
            "837c621da5774d57abe29575f1b00b29",
            "a5f9e795fe5d4b15b51da816e8815689",
            "118ab70dd9c5480cb483a9934d206ab9",
            "5a836e671a3f4ac8bf6f025d33d42211",
            "77cb24b4de064e00bab1da1a4de6c26c"
          ]
        },
        "outputId": "2b9a375c-730f-4e88-c0d5-bf5c02bf4b23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "918e607cb8354222a28a044286bd09f0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function to transform the image into a tensor\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return preprocess(image).unsqueeze(0)\n",
        "\n",
        "# Function to create image embeddings\n",
        "def create_image_embedding(image_path):\n",
        "    try:\n",
        "        input_tensor = preprocess_image(image_path)\n",
        "        with torch.no_grad():\n",
        "            embeddings = model(input_tensor)# ebedding important line\n",
        "        return embeddings.squeeze().numpy()\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "4a8gOBTRdALV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir images"
      ],
      "metadata": {
        "id": "ev6x8nXVeAIx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create python function where we provide image url and imag_name then it save in images folder\n",
        "\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def save_image_from_url(image_url, image_name):\n",
        "  \"\"\"\n",
        "  Downloads an image from a URL and saves it to the 'images' folder.\n",
        "\n",
        "  Args:\n",
        "    image_url: The URL of the image to download.\n",
        "    image_name: The name of the file to save the image as.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    if not os.path.exists(\"images\"):\n",
        "      os.makedirs(\"images\")\n",
        "\n",
        "    image_path = os.path.join(\"images\", image_name)\n",
        "\n",
        "    response = requests.get(image_url, stream=True)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    with open(image_path, 'wb') as file:\n",
        "      for chunk in response.iter_content(chunk_size=8192):\n",
        "        file.write(chunk)\n",
        "\n",
        "    print(f\"Image saved to: {image_path}\")\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading image: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "fA0O2fereLND"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_image_from_url(\"https://avatars.githubusercontent.com/u/110218203?v=4\",\"Fatima1.jpg\")\n",
        "save_image_from_url(\"https://media.licdn.com/dms/image/v2/D4E03AQG8lxYDv2d_jA/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1682492456789?e=2147483647&v=beta&t=stZdt8wW0KOnBeYK4nHX6LWbZg_bLO0weYdlulk5DtI\", \"Fatima2.jpg\")\n",
        "\n",
        "save_image_from_url(\"https://media.licdn.com/dms/image/v2/D4D03AQHOEoAtPmDaag/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1665588212273?e=1742428800&v=beta&t=6jm6CFLEiXeVtYXuDD8DRBH4zXL98xv0_ijNvLwyUDk\",\"Baba1.jpg\")\n",
        "save_image_from_url(\"https://scontent.fkhi2-2.fna.fbcdn.net/v/t1.6435-9/133112738_3557048331079530_5118216550695975604_n.jpg?_nc_cat=109&ccb=1-7&_nc_sid=833d8c&_nc_eui2=AeGh9UnkEA2ppIsdqiNQNeMehWObNuQ0boqFY5s25DRuiphokGWReadtESFMzFnjq5EyMAxGPtQWOu5N2tMN_QdQ&_nc_ohc=_S8umDPZBTQQ7kNvgGWqOsk&_nc_oc=AdhF2uaTU49Tfs8Xu9EQfyB7jraOK8ePU3B6NkNh4gRs5svZYc9Q8WkTQ4Zkx4rasyo&_nc_zt=23&_nc_ht=scontent.fkhi2-2.fna&_nc_gid=A1hg2gtYgDWcRCwKLs5aX9g&oh=00_AYCJVsCfnK8JmLkzOX-YpHpKimPU8owiD26rhuBAkRFnuA&oe=67AAF14B\",'Baba2.jpg')\n",
        "\n",
        "save_image_from_url(\"https://media.licdn.com/dms/image/v2/D4D03AQEBa7TZt4yOPw/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1665673062013?e=1742428800&v=beta&t=XBtWZkuF010UVmY1Gd-DTa9Axyhx6-NTJY6tQaE7Cj8\",'Mama1.jpg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g0FWGqJfS4F",
        "outputId": "2d44516d-837b-489e-9413-d6c093b61df8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image saved to: images/Fatima1.jpg\n",
            "Image saved to: images/Fatima2.jpg\n",
            "Image saved to: images/Baba1.jpg\n",
            "Image saved to: images/Baba2.jpg\n",
            "Image saved to: images/Mama1.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save_image_from_url(\"data:image/jpeg;base64,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\",\"q2.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU0zZ224f7q-",
        "outputId": "b8428d9e-6d32-4204-a005-3b644e8320aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading image: No connection adapters were found for 'data:image/jpeg;base64,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'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "image_path = \"./images/Fatima1.jpg\"\n",
        "q2 = create_image_embedding(image_path)\n",
        "\n",
        "# 'embedding' now contains a dense vector representation of the image\n",
        "print(\"Image Embedding Shape:\", q2.shape)\n",
        "print(\"Image Embedding:\", q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL_ISPWBg9-m",
        "outputId": "f631c59a-b93a-4031-8131-34006bb375b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Embedding Shape: (512,)\n",
            "Image Embedding: [ 2.33396906e-02 -1.97558827e-03 -6.59882426e-02  9.94308367e-02\n",
            "  5.68067841e-02  4.13036346e-02 -5.57346791e-02  2.69905236e-02\n",
            " -4.22182418e-02  1.57304108e-02  2.73002721e-02  3.22358459e-02\n",
            " -4.28939350e-02  1.69546418e-02  5.86699806e-02 -6.57564178e-02\n",
            "  3.31830606e-02  9.67113599e-02  3.53259481e-02 -4.83786725e-02\n",
            " -6.13883622e-02  3.06106396e-02  5.56395054e-02 -3.70847210e-02\n",
            " -6.77524460e-03 -2.92252959e-03  4.13161777e-02 -1.20267747e-02\n",
            "  5.41126467e-02  3.21275182e-03 -1.95411108e-02 -2.11430825e-02\n",
            " -1.63891837e-02  5.82626574e-02 -2.45407559e-02 -3.99103239e-02\n",
            " -3.89122367e-02 -2.36135651e-03 -7.19788447e-02  6.47758469e-02\n",
            " -3.45490538e-02 -1.23079028e-02 -1.99142601e-02 -7.92307127e-03\n",
            "  3.09956800e-02 -1.09178002e-03 -3.23415454e-03  1.36373313e-02\n",
            " -1.10326968e-01 -7.49271177e-03  3.21916454e-02 -9.25628841e-03\n",
            "  7.47417510e-02  2.53559207e-03 -6.72270432e-02 -3.63197215e-02\n",
            " -5.06171957e-02  4.68680188e-02 -1.20043997e-02  3.50923091e-02\n",
            " -1.28134899e-02 -1.33622671e-02 -1.03805838e-02 -1.20301340e-02\n",
            " -1.68237574e-02 -5.56510035e-03 -6.02468103e-03 -1.15147121e-02\n",
            " -3.61570157e-02 -2.84238849e-02  2.14967150e-02  1.65997108e-03\n",
            " -2.08360832e-02 -3.73139745e-03  6.72848970e-02 -4.77219746e-02\n",
            "  2.13393178e-02  2.92885373e-03  4.98513877e-02  1.99587084e-02\n",
            "  5.70681095e-02  6.17382675e-02 -3.40126717e-05 -2.01109983e-02\n",
            " -1.54455276e-02  7.60038123e-02 -3.72751313e-03  4.61175442e-02\n",
            " -3.13050821e-02 -2.25372836e-02  1.33850677e-02 -1.29588787e-02\n",
            "  2.74115475e-03 -5.86726479e-02  2.98731942e-02  2.34414190e-02\n",
            "  2.23422125e-02  5.30715473e-02 -4.21860479e-02  4.02832180e-02\n",
            "  7.14182341e-03  2.12980974e-02  4.11218852e-02 -4.15338716e-03\n",
            "  1.86216310e-02  1.54256476e-02 -3.45041752e-02 -2.76440214e-02\n",
            " -1.85232684e-02 -8.88195038e-02  6.84592947e-02 -1.34762435e-04\n",
            " -4.66231145e-02 -2.61203013e-02  7.85887614e-02  7.72715956e-02\n",
            "  5.17658554e-02  1.04281213e-02 -5.00541478e-02  5.95287234e-02\n",
            " -6.43363455e-03  1.03278428e-01 -5.49989101e-03 -1.69797689e-02\n",
            " -1.67676322e-02 -9.91721526e-02 -3.52106653e-02 -1.04307001e-02\n",
            "  4.02535498e-02 -4.00695065e-03 -9.53746773e-03 -1.86717156e-02\n",
            "  1.07941516e-01 -1.54854339e-02 -4.65551354e-02 -6.45824568e-03\n",
            " -3.03361635e-03 -5.99724054e-02  4.46528681e-02  5.91563396e-02\n",
            "  1.19887125e-02  6.76763132e-02 -2.41255574e-03  3.83102521e-02\n",
            "  2.49510240e-02  1.06948381e-02 -7.13552833e-02  4.84937169e-02\n",
            "  1.27444640e-02 -4.18727659e-02 -2.37360559e-02  4.23413739e-02\n",
            " -7.55247548e-02  3.65140401e-02  8.26663673e-02 -8.60808790e-02\n",
            " -6.22112397e-03  7.21876789e-03 -1.15212193e-02 -7.28936344e-02\n",
            "  2.32611690e-02  9.48904455e-02  3.49054262e-02  3.84394415e-02\n",
            "  6.44313321e-02  8.45308453e-02 -9.08322781e-02  1.88817363e-02\n",
            " -9.25079659e-02  1.11410283e-01 -5.78723662e-02  7.60519819e-04\n",
            "  3.38264331e-02 -1.97756011e-03 -1.26029216e-02 -6.25267299e-03\n",
            "  1.41229136e-02 -2.35684458e-02 -7.60218129e-02  2.74085198e-02\n",
            "  2.52079237e-02 -2.13262253e-02  6.10349048e-03  4.06302959e-02\n",
            "  4.44266535e-02 -2.15954501e-02  1.02036577e-02 -3.29570696e-02\n",
            "  7.68573806e-02  6.72680512e-02 -1.81865543e-02  3.65577750e-02\n",
            "  1.38755307e-01 -2.89387722e-03  1.60740893e-02  1.56906303e-02\n",
            " -4.73255618e-03  6.41415501e-03  2.63567735e-02 -2.64234990e-02\n",
            "  7.41053149e-02 -1.04291216e-01  6.98462501e-02 -2.05683205e-02\n",
            " -8.55103787e-03 -4.83342819e-03  3.10676135e-02  2.02092659e-02\n",
            "  2.02369764e-02  5.56218028e-02 -5.32082729e-02 -2.78467331e-02\n",
            " -6.09592721e-02  4.09948118e-02  2.78884955e-02  1.62568875e-02\n",
            "  1.50898444e-02  2.62021739e-02 -7.50006735e-02 -1.65240616e-02\n",
            " -4.21930924e-02 -2.20235363e-02  3.90840545e-02 -4.77597825e-02\n",
            "  2.64356434e-02  3.32157011e-03  3.99893709e-02  2.92764157e-02\n",
            " -1.53438328e-02  1.50398267e-02  1.26404266e-04  8.51594284e-02\n",
            "  9.85448435e-03 -5.34286462e-02 -3.76296341e-02  1.59880724e-02\n",
            " -2.66820826e-02 -2.96914857e-02  5.93249574e-02 -1.10268844e-02\n",
            " -8.92567933e-02 -3.72473523e-02 -2.13751327e-02  2.69250348e-02\n",
            "  5.80507703e-02  8.44276994e-02 -7.96288066e-03 -2.32547037e-02\n",
            "  1.34764472e-02  5.54326475e-02  2.11051963e-02  1.01366103e-01\n",
            "  7.06728920e-02 -2.36656126e-02 -7.54604349e-03 -2.19286941e-02\n",
            " -9.21431091e-03  1.73327141e-02 -2.02114135e-02  7.92524405e-03\n",
            " -1.06757749e-02 -1.17589859e-02 -2.07514949e-02 -3.16691659e-02\n",
            " -8.20023715e-02 -2.78083626e-02  6.41538799e-02 -3.48372906e-02\n",
            " -5.75258546e-02 -3.74832191e-02 -4.49541621e-02  4.08398062e-02\n",
            "  1.41605046e-02 -2.11689882e-02 -2.21105199e-02 -2.72004232e-02\n",
            " -9.26757697e-03  5.37893921e-03  2.71575134e-02  5.66599071e-02\n",
            "  2.22122259e-02  5.71750514e-02 -8.57278779e-02 -2.75463201e-02\n",
            " -3.68331303e-03  4.58351113e-02  6.99179247e-02  3.31221498e-03\n",
            " -4.09538709e-02 -5.68688735e-02  9.70467459e-03 -6.24882728e-02\n",
            "  8.96271169e-02  1.50762079e-02  4.28604754e-03  3.62748629e-03\n",
            " -2.76074577e-02  6.71227500e-02  6.68711439e-02  1.34633724e-02\n",
            "  8.32788348e-02 -4.11771284e-03 -2.98727266e-02  2.72366367e-02\n",
            " -2.74548563e-03 -5.95472753e-02  4.86757979e-03  1.00839650e-02\n",
            " -4.01355252e-02  3.25149037e-02 -9.83080789e-02  6.15824200e-02\n",
            "  7.89820030e-02  5.14429510e-02  1.08258106e-01  3.87679525e-02\n",
            "  3.11735310e-02 -4.94454242e-02  6.88239485e-02  3.22398394e-02\n",
            "  8.49624574e-02  1.40323527e-02 -2.46518888e-02 -6.45428896e-03\n",
            " -1.81372056e-03 -2.99109910e-02 -1.10910935e-02  4.58599702e-02\n",
            "  3.51361819e-02  7.80798495e-02  4.50467831e-03  6.26697838e-02\n",
            " -4.56469990e-02  6.50568157e-02 -2.85568219e-02  2.27362122e-02\n",
            " -4.52434309e-02  3.02183032e-02 -3.17365490e-02  3.33484784e-02\n",
            "  2.55879182e-02 -2.11267620e-02  3.79828550e-02  3.92348021e-02\n",
            "  4.98799160e-02 -2.02462934e-02 -7.00201094e-02  3.63000184e-02\n",
            "  6.12513907e-02 -1.32490210e-02 -1.76541694e-02 -4.47636917e-02\n",
            " -7.71574378e-02  3.10815987e-03  1.13899878e-03  4.58134487e-02\n",
            "  3.37003767e-02 -1.85478572e-02 -5.91580085e-02  5.67026995e-03\n",
            "  3.61867361e-02 -4.04885076e-02 -8.77759308e-02  2.05062199e-02\n",
            " -5.40149286e-02 -1.94603279e-02  1.13004679e-02  3.99696417e-02\n",
            "  9.77776013e-03 -5.06423302e-02 -1.14405630e-02  1.76641233e-02\n",
            "  2.13605445e-02  1.69116668e-02  6.29198225e-03  6.83026612e-02\n",
            " -2.75407173e-02  1.84946656e-02 -5.07009262e-03 -8.72257128e-02\n",
            " -6.48139566e-02 -3.07178181e-02 -5.32061607e-02 -1.20651834e-02\n",
            "  3.43443942e-04 -8.79602656e-02 -5.59608340e-02 -4.22845222e-02\n",
            "  2.42217779e-02 -3.56523432e-02 -1.52595071e-02  9.37944278e-02\n",
            "  5.50233535e-02 -1.61626600e-02 -3.81874815e-02  1.67212298e-03\n",
            "  3.08759627e-03  8.98778141e-02  7.18402639e-02  3.45475748e-02\n",
            " -7.14397505e-02  5.72544746e-02  2.32678428e-02  1.64411291e-02\n",
            "  3.46738622e-02 -1.88047569e-02  5.55183366e-03 -3.15891281e-02\n",
            "  5.17986380e-02  5.92073891e-03  4.70435470e-02  1.17100484e-04\n",
            "  6.70181215e-03 -3.79279107e-02 -2.79751383e-02  8.38314649e-03\n",
            " -6.57602306e-03 -1.17585976e-02  5.11930417e-03 -6.76922947e-02\n",
            "  6.84782043e-02 -9.30001587e-03  5.71327610e-03  4.08590287e-02\n",
            " -9.72241089e-02  2.43568402e-02  9.75117274e-03 -3.85916121e-02\n",
            "  1.31193465e-02 -4.95524406e-02 -2.43022516e-02  3.97362597e-02\n",
            " -1.14847906e-02 -5.54615930e-02 -1.30014047e-02 -4.02283408e-02\n",
            " -4.30771895e-02 -1.17714383e-01 -4.05207686e-02  4.29851376e-02\n",
            " -4.62630875e-02  2.43106950e-02 -3.69587876e-02  7.20203593e-02\n",
            " -2.07205638e-02  8.26198421e-03 -8.11834335e-02  3.61517235e-03\n",
            "  2.72964332e-02 -1.03480957e-01 -1.18958345e-02  2.17760932e-02\n",
            " -3.68562862e-02 -2.02520378e-02  2.27472708e-02  5.53074293e-02\n",
            " -2.93543357e-02  7.63141587e-02  7.56094977e-02  4.51107919e-02\n",
            "  4.16009352e-02  4.42647263e-02  1.55528747e-02  2.05660239e-02\n",
            "  3.60014290e-02 -5.32862879e-02  1.90507897e-04 -3.71402986e-02\n",
            " -1.12575646e-02  7.43562654e-02 -1.28367441e-02 -6.64049014e-02\n",
            " -7.93691631e-03 -1.67217571e-02 -5.71067072e-02  3.25164944e-02\n",
            "  8.64325836e-02 -7.10385516e-02  6.73785135e-02  6.68760166e-02\n",
            "  3.19764167e-02  1.24159949e-02 -4.51088548e-02 -2.94956402e-03\n",
            "  1.38730397e-02 -2.61584911e-02 -7.64377862e-02  3.29206921e-02\n",
            "  2.48511061e-02 -1.27822198e-02 -3.81437540e-02 -1.17565570e-02\n",
            " -2.53641475e-02 -2.15491839e-02  5.87566793e-02 -4.61295433e-02\n",
            "  4.79373522e-02  4.15265895e-02  3.79699469e-02  1.83347668e-02\n",
            "  4.46023466e-03  3.00374944e-02  1.75811071e-02 -4.36196029e-02\n",
            " -3.99407232e-04  3.15306745e-02  1.89310405e-02  1.38023254e-02\n",
            " -2.08265148e-02 -6.18591011e-02  1.98751129e-02 -1.54896155e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"./images/Mama1.jpg\"\n",
        "cat2 = create_image_embedding(image_path)\n",
        "\n",
        "# 'embedding' now contains a dense vector representation of the image\n",
        "print(\"Image Embedding Shape:\", cat2.shape)\n",
        "print(\"Image Embedding:\", cat2)"
      ],
      "metadata": {
        "id": "xREy5jgFhDeM",
        "outputId": "9492dd42-77e9-4f6a-b822-fe1058daf5af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Embedding Shape: (512,)\n",
            "Image Embedding: [ 5.85408621e-02 -1.09868438e-03 -6.75724670e-02  6.39401898e-02\n",
            " -1.23365233e-02  4.21871319e-02 -2.46587098e-02  5.60998060e-02\n",
            " -2.02880315e-02 -1.01089403e-01 -1.36232739e-02  2.04500295e-02\n",
            "  9.75819118e-03 -4.77575790e-03  4.42324765e-02  2.36301478e-02\n",
            "  5.47326952e-02  1.63078692e-03 -1.66244972e-02 -9.91698205e-02\n",
            " -6.03894331e-02  4.28633904e-03  2.85973195e-02  1.33260700e-03\n",
            "  2.25316007e-02  1.88566167e-02  2.38239914e-02 -5.37987463e-02\n",
            "  7.30174687e-03  2.94888057e-02 -6.01074286e-02  3.75106744e-03\n",
            " -2.47812606e-02  2.79388595e-02 -1.17200382e-01  9.14315656e-02\n",
            "  9.81638441e-05  3.32520530e-02 -7.58338720e-02  6.83301454e-03\n",
            "  1.85503103e-02 -2.41057053e-02 -1.77563087e-03 -4.76593040e-02\n",
            "  3.50169465e-02  2.67957593e-03  3.48376147e-02  6.79845661e-02\n",
            " -1.06163666e-01  2.36568754e-04  6.10018559e-02  3.44888791e-02\n",
            "  3.01967151e-02 -1.66789144e-02 -1.02531135e-01  1.86084248e-02\n",
            " -8.40464793e-03  9.76172835e-03  4.46428806e-02 -7.13634817e-03\n",
            " -3.80513631e-02  3.58740576e-02 -8.60103127e-03 -4.44470323e-04\n",
            " -2.63171047e-02  5.50766774e-02 -1.12212151e-02  4.87222485e-02\n",
            "  2.29333248e-02 -5.96803287e-03  1.32103302e-02  3.08271237e-02\n",
            "  1.49456365e-02  9.46824366e-05  3.60437185e-02 -5.38170040e-02\n",
            " -7.10628927e-02 -1.62414033e-02 -5.98554835e-02 -5.55591332e-03\n",
            "  8.57063979e-02  2.74827965e-02  1.77764576e-02 -1.24881733e-02\n",
            "  1.26040226e-03  6.98017306e-04 -4.78142453e-03 -2.30868123e-02\n",
            "  1.00902487e-02 -6.17169123e-03  2.88626458e-02  7.70594031e-02\n",
            " -6.73987577e-03 -4.91914637e-02  7.52810910e-02 -4.24633094e-04\n",
            " -2.67786309e-02 -3.20581123e-02 -1.06622010e-01 -2.98299529e-02\n",
            "  3.30169946e-02 -2.79454254e-02 -3.43695618e-02  1.62983872e-02\n",
            " -1.63973700e-02  3.90801132e-02  6.89632967e-02 -4.51951660e-02\n",
            " -4.25262041e-02 -4.16840985e-02  2.76146811e-02 -3.33045889e-03\n",
            " -1.07953455e-02 -1.34086544e-02  4.02668789e-02 -4.20177057e-02\n",
            "  1.54409213e-02 -5.46034388e-02 -1.94490347e-02  3.55123207e-02\n",
            " -4.33307625e-02  6.40960187e-02  1.04937851e-02 -1.38944611e-01\n",
            " -6.29512444e-02 -9.53950062e-02  2.73684436e-03  2.53648125e-02\n",
            "  3.64056490e-02 -6.23619631e-02  1.49544114e-02 -2.33307900e-03\n",
            " -1.49102248e-02 -1.06550753e-02 -1.02243096e-01  3.75885032e-02\n",
            " -8.00697133e-03 -3.24495658e-02  3.58489938e-02  5.95008768e-02\n",
            "  3.15679051e-02  3.81403640e-02 -1.86659507e-02  1.00625334e-02\n",
            "  7.69223049e-02 -5.43223992e-02  3.80250253e-02 -2.82808114e-02\n",
            " -4.85907483e-04  6.66166481e-04  2.90784482e-02 -6.66320920e-02\n",
            " -6.95638806e-02  4.03830828e-03  8.46821070e-02  2.69806441e-02\n",
            " -2.01157783e-03  2.90877139e-03  5.79386614e-02 -1.72034446e-02\n",
            "  7.43144825e-02  2.15517972e-02  1.07302135e-02  7.04735192e-03\n",
            "  2.24793218e-02  6.07901951e-03 -8.23008046e-02  1.89678986e-02\n",
            " -8.79193842e-02 -2.77649891e-03 -1.35299927e-02  4.46772128e-02\n",
            "  1.39970891e-02  3.72571945e-02  1.12821974e-01  7.24711493e-02\n",
            "  3.72318104e-02 -5.85471094e-02 -3.62129286e-02  3.97557430e-02\n",
            " -3.97919230e-02 -3.70231010e-02 -7.44633749e-02 -2.29814611e-02\n",
            " -1.42185958e-02  6.00784924e-03  2.94567738e-02  8.22365731e-02\n",
            "  7.68809095e-02  5.67571931e-02  2.09054928e-02 -6.93073198e-02\n",
            "  8.24262053e-02 -3.10548060e-02  5.03101870e-02 -9.52762645e-03\n",
            "  2.24872623e-02  5.10871187e-02 -8.40395913e-02  1.34809287e-02\n",
            "  2.59494334e-02 -5.01590632e-02  5.46920858e-02  5.15503436e-02\n",
            "  4.71602380e-02  1.19470535e-02  8.02332088e-02 -1.07598938e-02\n",
            "  9.75842588e-03  2.07531098e-02  1.72534510e-02  2.20633391e-02\n",
            " -5.82274189e-03  5.96930124e-02  2.23354083e-02 -2.22659986e-02\n",
            " -2.75717136e-02  7.61230290e-03  1.79856492e-03 -1.41619435e-02\n",
            " -7.17406049e-02 -1.10200569e-01 -9.63055715e-02 -4.77973856e-02\n",
            "  2.23944914e-02  1.20420870e-03 -4.11247090e-02  4.34400253e-02\n",
            " -7.68837929e-02 -1.18444245e-02  7.08071468e-03  4.49941084e-02\n",
            " -9.44518019e-04 -8.13921243e-02 -2.55200267e-02 -3.85225564e-02\n",
            "  8.95277504e-03 -1.08198039e-02  1.16052579e-04 -6.46645352e-02\n",
            "  4.92615532e-03 -3.67948040e-02  2.35643741e-02 -3.00138164e-02\n",
            "  8.42485800e-02  1.18376382e-01  3.12225334e-02  2.76522827e-03\n",
            " -2.32884530e-02  4.19792980e-02 -4.98220213e-02  2.74111684e-02\n",
            " -1.59558617e-02 -4.17100033e-03 -1.64608881e-02 -2.98539419e-02\n",
            "  1.57242678e-02 -4.40213084e-02  2.46506854e-04  5.72979786e-02\n",
            " -1.81576721e-02 -2.78536696e-02  4.55555413e-03 -1.06868766e-01\n",
            " -5.02573773e-02 -5.95437456e-03  1.88241433e-02 -5.21553867e-02\n",
            " -5.57990819e-02 -1.63423177e-02 -4.33113687e-02  2.18773484e-02\n",
            "  4.35539782e-02 -3.65251340e-02 -1.18813990e-02 -3.66283059e-02\n",
            " -3.44343949e-04 -2.58783158e-02  1.48430746e-02 -1.07415142e-02\n",
            " -1.59407835e-02  6.97640469e-04 -8.08581361e-04 -9.69737321e-02\n",
            " -8.30775350e-02  3.00102830e-02 -8.07426497e-03  6.14086352e-03\n",
            " -1.86312571e-02 -4.82737608e-02 -1.29576800e-02 -6.15207888e-02\n",
            "  2.59512700e-02 -6.28184155e-03 -2.34307963e-02 -9.83145535e-02\n",
            "  1.45238861e-02  4.63486463e-02  2.71160193e-02 -2.68311054e-03\n",
            "  5.89824468e-02 -9.57667083e-02  1.36447679e-02 -1.48843993e-02\n",
            "  2.82215830e-02  8.43555946e-03 -7.60753378e-02 -3.61709967e-02\n",
            "  7.37351775e-02  7.28610605e-02  1.42087066e-03  5.20101488e-02\n",
            "  1.66363437e-02  6.48204982e-02  4.09504250e-02 -1.03579061e-02\n",
            " -1.76711716e-02  3.54716927e-02  1.56670157e-02  3.39968428e-02\n",
            "  1.00067683e-01 -2.06149258e-02 -5.55584393e-02  9.17376764e-03\n",
            "  2.03174558e-02  3.59608568e-02  1.09588718e-02  7.22817406e-02\n",
            "  4.06026803e-02  2.32290328e-02  6.61691576e-02  1.02843940e-02\n",
            " -1.14507542e-03  3.46760824e-02 -3.95409167e-02 -1.23286322e-02\n",
            " -4.60936921e-03 -4.93977480e-02  9.09886435e-02  1.07932268e-02\n",
            " -5.31210518e-03 -2.71100886e-02  2.58814525e-02 -2.87859607e-02\n",
            "  4.75297533e-02 -6.55926168e-02  2.81841476e-02  4.49091308e-02\n",
            "  1.13454489e-02 -7.20472559e-02 -2.10975260e-02  1.88373663e-02\n",
            " -4.12826277e-02 -7.02247247e-02 -4.73171510e-02  5.03054708e-02\n",
            "  3.29600088e-02 -2.84428503e-02 -1.23030785e-02  5.54753374e-03\n",
            "  1.19692357e-02 -5.24090827e-02 -2.59309374e-02  7.18313316e-03\n",
            " -4.37280200e-02  2.85649095e-02  4.65315059e-02 -4.50807549e-02\n",
            "  4.14946824e-02 -6.37513548e-02 -3.93889844e-02 -2.87672691e-02\n",
            "  2.08874159e-02 -3.10238395e-02  2.86716428e-02  9.01557729e-02\n",
            "  6.26721978e-03  7.46163912e-03 -4.53396663e-02 -2.52710134e-02\n",
            " -8.11022222e-02 -8.00278410e-02 -4.49833311e-02  2.98242699e-02\n",
            "  2.14932393e-02 -2.60777865e-02 -4.95651104e-02  3.42494026e-02\n",
            "  1.76437870e-02 -9.88939404e-02  2.11181287e-02  3.06243915e-02\n",
            "  1.14884777e-02 -3.80229056e-02 -9.00910795e-02  7.16554048e-03\n",
            "  4.16157767e-02  5.17462380e-02  2.17829458e-02  4.39198688e-02\n",
            " -5.40415458e-02  6.99567422e-02 -1.27667803e-02  2.15223599e-02\n",
            " -9.71141458e-03 -4.64907698e-02 -2.74346443e-03 -4.18070033e-02\n",
            "  2.30550785e-02  4.46791723e-02  6.29812181e-02 -3.05966884e-02\n",
            "  3.23603153e-02  4.06230800e-03  1.64356735e-02 -2.54099295e-02\n",
            "  2.05492824e-02  2.58199908e-02  3.75261270e-02 -8.47363174e-02\n",
            " -5.45362607e-02  1.21383080e-02 -1.54694207e-02 -3.55076678e-02\n",
            " -8.80846307e-02 -2.58679800e-02 -5.65097295e-03  5.46684265e-02\n",
            "  4.35390845e-02 -4.28973064e-02  5.97249065e-03 -2.91758566e-03\n",
            " -6.84648529e-02 -6.82892054e-02 -7.49601144e-03  2.07230356e-03\n",
            " -1.47942640e-02 -2.21547280e-02  4.55208980e-02 -3.67473625e-02\n",
            " -3.67264263e-02  6.23030495e-03  3.49989943e-02  6.44948632e-02\n",
            " -4.69887219e-02  3.36818658e-02 -6.06223270e-02  5.32346182e-02\n",
            "  1.49404379e-02 -1.84646994e-02 -8.70728195e-02 -1.41373239e-02\n",
            " -5.26588187e-02 -7.50963390e-02 -1.44609949e-02  1.90398972e-02\n",
            "  4.17457037e-02  4.61881980e-02  3.81518602e-02  6.82767183e-02\n",
            " -1.38101857e-02 -1.74156912e-02  4.18427866e-03 -1.17769530e-02\n",
            "  3.95322554e-02 -5.38238101e-02 -1.06136864e-02  1.47167854e-02\n",
            "  1.21472180e-02  5.44460118e-02  4.74561676e-02 -5.84657080e-02\n",
            "  4.68713827e-02  4.49611805e-03 -1.07672527e-01 -4.72681224e-02\n",
            "  7.38482401e-02  2.92843524e-02  3.15900631e-02 -3.33995465e-03\n",
            "  2.76333224e-02 -3.69202830e-02  1.90531928e-02 -4.14640456e-02\n",
            " -5.26677743e-02  4.50969599e-02 -8.15363303e-02  2.79379822e-02\n",
            "  1.17532851e-03  4.19461653e-02 -9.64786764e-03 -3.75062339e-02\n",
            "  1.26139019e-02 -7.36227781e-02  3.88731435e-02 -4.45528217e-02\n",
            " -1.15601700e-02  2.33997907e-02  1.18996702e-01  1.07248931e-03\n",
            " -1.00489333e-02 -1.28071615e-02 -1.11234933e-02 -7.21765589e-03\n",
            "  1.77846272e-02  6.08989932e-02  5.69520742e-02  6.24932814e-03\n",
            " -1.28569892e-02  3.43501242e-03  4.20089178e-02 -4.28657793e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fatima1 = create_image_embedding(\"./images/Fatima1.jpg\")\n",
        "fatima2 = create_image_embedding(\"./images/Fatima2.jpg\")\n",
        "baba1 = create_image_embedding(\"./images/Baba1.jpg\")\n",
        "baba2 = create_image_embedding(\"./images/Baba2.jpg\")\n",
        "mama1 = create_image_embedding(\"./images/Mama1.jpg\")\n"
      ],
      "metadata": {
        "id": "nupxvZqe61kj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq milvus-lite\n",
        "\n",
        "!pip install -Uq pymilvus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYLF1JmW7XYw",
        "outputId": "0bfb8dec-9a53-4277-e266-f45f639ceff8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import MilvusClient\n",
        "client = MilvusClient(\"./milvus_demo.db\")"
      ],
      "metadata": {
        "id": "DyghQke97iKR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import MilvusClient\n",
        "import numpy as np\n",
        "\n",
        "client = MilvusClient(\"./milvus_demo.db\")\n",
        "client.create_collection(\n",
        "    collection_name=\"images\",\n",
        "    dimension=512)  # The vectors we will use in this demo has 384 dimensions"
      ],
      "metadata": {
        "id": "vsXGZGlY7nqh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{\"id\": 1, \"person_name\": \"Fatima Khan\", \"vector\": fatima1},\n",
        "    {\"id\": 2, \"person_name\": \"Fatima Khan\", \"vector\": fatima2},\n",
        "    {\"id\": 3, \"person_name\": \"Abdul Faheem\", \"vector\": baba1},\n",
        "    {\"id\": 4, \"person_name\": \"Abdul Faheem\", \"vector\": baba2},\n",
        "    {\"id\": 5, \"person_name\": \"Gul-E-Nargis\", \"vector\": mama1}]"
      ],
      "metadata": {
        "id": "tM6ZuT0o74ej"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.insert(\n",
        "    collection_name=\"images\",\n",
        "    data=data)"
      ],
      "metadata": {
        "id": "NsjfnvqP8qes"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.search(\n",
        "    collection_name=\"images\",\n",
        "    data=[fatima2],\n",
        "    limit=1,\n",
        "    output_fields=[\"id\", \"person_name\"],\n",
        ")\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRpYzDDM82TV",
        "outputId": "4995b687-d79c-4479-835e-493792582e2d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: [\"[{'id': 2, 'distance': 1.0, 'entity': {'person_name': 'Fatima Khan', 'id': 2}}]\"] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.search(\n",
        "    collection_name=\"images\",\n",
        "    data=[mama1],\n",
        "    limit=1,\n",
        "    output_fields=[\"id\", \"person_name\"],\n",
        ")\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_bXXU6O9Stx",
        "outputId": "c3cdf1ab-a40c-4a69-b5c7-062d69c00e78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: [\"[{'id': 5, 'distance': 0.9999998807907104, 'entity': {'person_name': 'Gul-E-Nargis', 'id': 5}}]\"] \n"
          ]
        }
      ]
    }
  ]
}