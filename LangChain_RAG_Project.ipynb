{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN66/EVLpCeXUaFghTbIuVK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatimakgeneng/LangChain-RAG-Project/blob/main/LangChain_RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "n4KjtjXHSVsq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install -q -U python-dotenv langchain pinecone-client google-generativeai openai tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve and set the environment variables\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_ENVIRONMENT'] = userdata.get('PINECONE_ENVIRONMENT')\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "Ozh1fjZ-RMFN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'], environment=os.environ['PINECONE_ENVIRONMENT'])"
      ],
      "metadata": {
        "id": "NHFhJlnxoztw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to an existing index\n",
        "index_name = \"gemini-rag-index\"  # Replace with your actual index name\n",
        "index = pc.Index(index_name)\n",
        "print(f\"Successfully connected to index: {index_name}\")"
      ],
      "metadata": {
        "id": "ds3uKipsp7lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5639708-289d-4418-e036-8ac96e529ea9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to index: gemini-rag-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain-google-genai\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",\n",
        "    google_api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "ihz0p83zp7UJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain-community\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents\n",
        "loader = TextLoader(\"/content/document.txt\")  # Replace with your file\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "HCa_vvE5sVeB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings.embed_query(doc.page_content)\n",
        "    # Pass metadata as a dictionary\n",
        "    index.upsert([(doc.metadata[\"source\"], vector, {\"text\": doc.page_content})])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ScoJXpatvcP",
        "outputId": "399597d3-0747-48f8-b484-e1a95cacb2e5"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:06<00:00,  1.15it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "retriever = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings, text_key=\"text\")"
      ],
      "metadata": {
        "id": "yQF6vA03tvN2"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U langchain-google-genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(api_key=os.environ.get(\"GOOGLE_API_KEY\"),model=\"gemini-1.5-flash\", temperature=0.7)"
      ],
      "metadata": {
        "id": "kKf8L3lbyN0-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores.base import VectorStoreRetriever\n",
        "\n",
        "# Create a VectorStoreRetriever from your Pinecone index\n",
        "retriever = VectorStoreRetriever(vectorstore=Pinecone.from_existing_index(index_name=index_name, embedding=embeddings, text_key=\"text\"))\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_model,\n",
        "    chain_type=\"stuff\",  # Other options: \"map_reduce\", \"refine\"\n",
        "    retriever=retriever)"
      ],
      "metadata": {
        "id": "B7zmqMFIyW3k"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what happens when you integrate the AI in genetics\"\n",
        "response = qa_chain.run(query)\n",
        "print(f\"Fatima's Question: {query}\")\n",
        "print()\n",
        "print(f\"LLM's Response: {response}\")"
      ],
      "metadata": {
        "id": "QUp1uKGHyWjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1408a6-73bd-4130-977c-28c4446d25f1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fatima's Question: what happens when you integrate the AI in genetics\n",
            "\n",
            "LLM's Response: Integrating AI into genetics has the potential to improve human health and solve critical challenges.  However, responsible innovation, collaboration, and regulation are crucial for its success.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}